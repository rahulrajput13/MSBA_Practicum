{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a56868",
   "metadata": {},
   "source": [
    "If you face the following error: 'IndexError: single positional indexer is out-of-bounds', try skipping the particular laptop number at which the error came by manually changing the value of i to the next one in the for loop in Section 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19889cf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "from pymongo import MongoClient\n",
    "from json import dumps\n",
    "\n",
    "def main():\n",
    "\n",
    "############################## SECTION 1 ########################################################################\n",
    "#### Loading files from Local and parse into BeautifulSoup objects - CHANGE DIRECTORY accordingly\n",
    "    directory = '/Users/rahulrajput/Desktop/MSBA/Winter/462 - Practicum/NVIDIA Laptops'\n",
    "    output_text = \"\"\n",
    "    cnt = 1\n",
    "    final_laptop_outputs = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        path = os.path.join(directory, filename)\n",
    "\n",
    "        with open(path, \"rb\") as file:\n",
    "            final_laptop_soup = BeautifulSoup(file)\n",
    "\n",
    "        final_laptop_outputs[\"Laptop_\" + str(cnt)] = final_laptop_soup\n",
    "        print(cnt)\n",
    "        cnt = cnt + 1\n",
    "\n",
    "############################## SECTION 2 ########################################################################\n",
    "#### Storing each Laptop Data into a Master Dictionary    \n",
    "    NVIDIA_Laptops = [] # Master dictionary for Qualcomm\n",
    "\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    database = client['Practicum']\n",
    "    collection = database['NVIDIA_Laptops']    \n",
    "\n",
    "    to_store = ['Title','Pricing','Features_Overview','About_Item','Suggested_Laptops','QnA','Reviews']\n",
    "    Laptop_NVIDIA_Info = dict.fromkeys(to_store) # Dictionary to store data from each laptop\n",
    "\n",
    "    for i in range(41,len(final_laptop_outputs)+1):\n",
    "\n",
    "        # Selecting Title\n",
    "        Laptop_NVIDIA_Info['Title'] = get_title(final_laptop_outputs[\"Laptop_\" + str(i)])\n",
    "        #print(Laptop_Qualcomm_Info['Title'])\n",
    "\n",
    "        # Selecting Prices\n",
    "        Laptop_NVIDIA_Info['Pricing'] = get_price(final_laptop_outputs[\"Laptop_\" + str(i)])\n",
    "        #print(Laptop_Qualcomm_Info['Pricing'])\n",
    "\n",
    "        # Selecting Features Overview Text\n",
    "        Laptop_NVIDIA_Info['Features_Overview'] = get_features_overview(final_laptop_outputs[\"Laptop_\" + str(i)])    \n",
    "        #print(Laptop_Qualcomm_Info['Features_Overview'])\n",
    "\n",
    "        # Selecting About Item Text\n",
    "        Laptop_NVIDIA_Info['About_Item'] = get_about_item(final_laptop_outputs[\"Laptop_\" + str(i)])\n",
    "        #print(Laptop_Qualcomm_Info['About_Item'])\n",
    "\n",
    "        # Selecting text from Suggested Items\n",
    "        Laptop_NVIDIA_Info['Suggested_Laptops'] = get_comparisons(final_laptop_outputs[\"Laptop_\" + str(i)])        \n",
    "        #print(Laptop_Qualcomm_Info['Suggested_Laptops'])\n",
    "\n",
    "        # Selecting Reviews text\n",
    "        first_url = get_first_reviews_url(final_laptop_outputs[\"Laptop_\" + str(i)])\n",
    "\n",
    "        review_page_soups = []\n",
    "        test = get_all_review_soups(first_url, review_page_soups)                               \n",
    "\n",
    "        All_Reviews = []\n",
    "        for soup in review_page_soups:\n",
    "            all_types_reviews = dict.fromkeys(['Overview','Top_Reviews','All_Reviews'])\n",
    "            \n",
    "            all_types_reviews['Overview'] = get_reviews_table(soup)\n",
    "            all_types_reviews['Top_Reviews'] = get_top_reviews(soup)\n",
    "            all_types_reviews['All_Reviews'] = get_reviews(soup)\n",
    "            \n",
    "            All_Reviews.append(all_types_reviews)\n",
    "\n",
    "        Laptop_NVIDIA_Info['Reviews'] = All_Reviews\n",
    "\n",
    "        # Selecting QnA text\n",
    "        first_url_qna = get_first_qna_url(final_laptop_outputs[\"Laptop_\" + str(i)])\n",
    "\n",
    "        qna_soups = []\n",
    "        soups_qna = get_all_qnas_soups(first_url_qna,qna_soups)\n",
    "\n",
    "        All_Questions = []\n",
    "        if soups_qna is not None:\n",
    "            for soup in soups_qna:\n",
    "                All_Questions.append(get_questions(soup))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        Laptop_NVIDIA_Info['QnA'] = All_Questions\n",
    "\n",
    "        \n",
    "        # Storing each laptop data\n",
    "        print(Laptop_NVIDIA_Info)\n",
    "        NVIDIA_Laptops.append(Laptop_NVIDIA_Info)\n",
    "        \n",
    "        doc = {\n",
    "            \"Title\": Laptop_NVIDIA_Info[\"Title\"],\n",
    "             \"Pricing\": Laptop_NVIDIA_Info[\"Pricing\"],\n",
    "             \"Features_Overview\": Laptop_NVIDIA_Info[\"Features_Overview\"],\n",
    "             \"About_Item\": Laptop_NVIDIA_Info[\"About_Item\"],\n",
    "             \"Suggested_Laptops\": Laptop_NVIDIA_Info[\"Suggested_Laptops\"],\n",
    "             \"QnA\": Laptop_NVIDIA_Info[\"QnA\"],\n",
    "             \"Reviews\": Laptop_NVIDIA_Info[\"Reviews\"]}\n",
    "       \n",
    "        collection.insert_one(doc)\n",
    "        \n",
    "        with open(f'NVIDIA_Data_{i}.csv', mode='w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=to_store)\n",
    "            writer.writeheader()\n",
    "            writer.writerow(Laptop_NVIDIA_Info)       \n",
    "\n",
    "############################## SECTION 3 ########################################################################\n",
    "#### Creating functions to extract information from Laptop Pages    \n",
    "    \n",
    "def get_title(soup_input):\n",
    "    if soup_input.select('span#productTitle'):\n",
    "        title_string = re.search('\\s*([a-zA-Z0-9].+)\\s*',soup_input.select('span#productTitle')[0].text)\n",
    "        title = title_string.group(1)\n",
    "        return title\n",
    "    else:\n",
    "        return None    \n",
    "\n",
    "\n",
    "def get_price(soup_input):\n",
    "    if soup_input.select('div#apex_desktop'):\n",
    "        for i in soup_input.select('div#apex_desktop'):\n",
    "            prices = re.findall('\\s*([0-9.]+)',i.text.strip())\n",
    "            return prices\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_features_overview(soup_input):\n",
    "    if soup_input.select('div#productOverview_feature_div table.a-normal.a-spacing-micro'):\n",
    "        \n",
    "        features_table = soup_input.select('div#productOverview_feature_div table.a-normal.a-spacing-micro')\n",
    "        features_df = pd.read_html(str(features_table))[0]\n",
    "        \n",
    "        dict_FO = features_df.to_dict()\n",
    "        new_dict_FO_keys = features_df.iloc[:,0]\n",
    "        new_dict_FO_values = features_df.iloc[:,1]\n",
    "        new_dict_FO = dumps(dict(zip(new_dict_FO_keys,new_dict_FO_values)))        \n",
    "        \n",
    "        return new_dict_FO\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_about_item(soup_input):\n",
    "    if soup_input.find('div',{'id':'feature-bullets'}):\n",
    "        details = soup_input.find('div',{'id':'feature-bullets'})\n",
    "        About = []\n",
    "        for li in details.find_all('li'):\n",
    "            About.append(li.text.strip())\n",
    "        return About\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def get_comparisons(soup_input, SI_output = None):\n",
    "     if soup_input.find(\"table\",{\"id\":\"HLCXComparisonTable\"}):\n",
    "        if SI_output is None:\n",
    "            SI_output = []        \n",
    "        \n",
    "        tbl = soup_input.find(\"table\",{\"id\":\"HLCXComparisonTable\"})\n",
    "        data_frame = pd.read_html(str(tbl))[0]\n",
    "        data_frame2 = data_frame.drop(labels=[1,2], axis=0).reset_index(drop=True)\n",
    "        \n",
    "        new_dict_SI_keys = data_frame2.iloc[:,0]\n",
    "        new_dict_SI_keys[0,0] = 'Name'\n",
    "        \n",
    "        for col in data_frame2.iloc[:, 1:]:\n",
    "            new_dict_SI_values = data_frame2.iloc[:,col]\n",
    "            new_dict_SI = dumps(dict(zip(new_dict_SI_keys,new_dict_SI_values)))            \n",
    "            SI_output.append(new_dict_SI)\n",
    "        \n",
    "        return SI_output\n",
    "     else:\n",
    "        return None\n",
    "    \n",
    "###### Reviews Section #######\n",
    "def get_first_reviews_url(soup_input):    \n",
    "    output_url = None\n",
    "    \n",
    "    if soup_input.find(\"table\",{\"id\":\"productDetails_detailBullets_sections1\"}):\n",
    "        table = soup_input.find(\"table\",{\"id\":\"productDetails_detailBullets_sections1\"})\n",
    "        data_frame = pd.read_html(str(table))[0]\n",
    "        asin = data_frame[data_frame.iloc[:,0] == 'ASIN']\n",
    "        final_asin = asin.iloc[0,1]\n",
    "        if final_asin is not None:\n",
    "            output_url = \"https://www.amazon.com/product-reviews/\" + final_asin\n",
    "            return output_url\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_all_review_soups(given_url,All_Review_Soups = None):   \n",
    "    \n",
    "    if given_url is not None:\n",
    "        if All_Review_Soups is None:\n",
    "            All_Review_Soups = []        \n",
    "\n",
    "        headers = {\n",
    "                'authority': 'fls-na.amazon.com',\n",
    "                'pragma': 'no-cache',\n",
    "                'method': 'GET',\n",
    "                'cache-control': 'no-cache',\n",
    "                'dnt': '1',\n",
    "                'upgrade-insecure-requests': '1',\n",
    "                'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8'\n",
    "        }    \n",
    "\n",
    "        session_reviews = requests.Session()\n",
    "        reviews_page = session_reviews.get(given_url, headers=headers)\n",
    "        print(\"The status code for website access is :\",reviews_page.status_code)\n",
    "        ######## STORE LAPTOP REVIEWS LOCALLY ############\n",
    "        with open(f\"NVIDIAReviews_{random.randint(0,10000)}.htm\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(reviews_page.text)\n",
    "        \n",
    "        review_soup = BeautifulSoup(reviews_page.text, 'html.parser')\n",
    "        #print(review_soup)\n",
    "\n",
    "        All_Review_Soups.append(review_soup)\n",
    "        #print(len(All_Review_Soups))\n",
    "\n",
    "        pagination_link = review_soup.select_one('ul.a-pagination li.a-last a')\n",
    "\n",
    "        if pagination_link is None:\n",
    "            print('done')\n",
    "            return All_Review_Soups    \n",
    "\n",
    "        next_url = \"https://www.amazon.com/\" + pagination_link['href']\n",
    "        print(next_url)\n",
    "\n",
    "        if next_url == given_url:\n",
    "            return All_Review_Soups\n",
    "\n",
    "        time.sleep(3)\n",
    "        return get_all_review_soups(next_url, All_Review_Soups)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_reviews(review_soup):\n",
    "    \n",
    "    if review_soup is not None:\n",
    "        # Getting all reviews\n",
    "        try:\n",
    "            all_reviews = []\n",
    "            for k in review_soup.select('div.a-section.review.aok-relative'):\n",
    "                all_reviews.append(k.text.strip())\n",
    "\n",
    "        except AttributeError:\n",
    "            print('No reviews')\n",
    "\n",
    "        return all_reviews    \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_reviews_table(review_soup):\n",
    "    if review_soup is not None:\n",
    "        if bool(review_soup.find(\"table\",{\"id\":\"histogramTable\"})) is True:\n",
    "            reviews_table = review_soup.find(\"table\",{\"id\":\"histogramTable\"})\n",
    "            data_frame_reviews = pd.read_html(str(reviews_table))[0]\n",
    "\n",
    "            new_dict_RT_keys = ['5 Star','4 Star','3 Star','2 Star','1 Star']\n",
    "            new_dict_RT_values = data_frame_reviews.iloc[:,2]\n",
    "            new_dict_RT = dumps(dict(zip(new_dict_RT_keys,new_dict_RT_values)))   \n",
    "\n",
    "            return new_dict_RT\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_top_reviews(review_soup):\n",
    "    if review_soup is not None:\n",
    "        if bool(review_soup.select(\"div#cm_cr-rvw_summary\")) is True:\n",
    "            try:        \n",
    "                top_reviews = []\n",
    "                for j in review_soup.select(\"div#cm_cr-rvw_summary\"):\n",
    "                    top_reviews.append(j.text.strip())\n",
    "\n",
    "            except AttributeError:\n",
    "                print('No top reviews')\n",
    "            \n",
    "            return top_reviews\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "###################################################################################################### \n",
    "\n",
    "##### QNA Section #####\n",
    "\n",
    "def get_first_qna_url(soup_input):\n",
    "    \n",
    "    if soup_input.select('div.a-section.cdQuestionAnswerBucket a.a-link-emphasis'):\n",
    "        for i in soup_input.select('div.a-section.cdQuestionAnswerBucket a.a-link-emphasis'):\n",
    "            qa_url = \"https://www.amazon.com\" + i['href']\n",
    "\n",
    "        return qa_url\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "All_QNA_Soups = []\n",
    "def get_all_qnas_soups(qa_url,All_QNA_Soups = None):\n",
    "        \n",
    "    if qa_url is not None:\n",
    "        if All_QNA_Soups is None:\n",
    "            All_QNA_Soups = []\n",
    "\n",
    "        headers = {\n",
    "            'authority': 'fls-na.amazon.com',\n",
    "            'pragma': 'no-cache',\n",
    "            'method': 'POST',\n",
    "            'cache-control': 'no-cache',\n",
    "            'dnt': '1',\n",
    "            'upgrade-insecure-requests': '1',\n",
    "            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "            'sec-fetch-site': 'none',\n",
    "            'sec-fetch-mode': 'navigate',\n",
    "            'sec-fetch-dest': 'document',\n",
    "            'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "        }\n",
    "\n",
    "        session_qna = requests.Session()\n",
    "        qna_page = session_qna.get(qa_url, headers=headers)\n",
    "        print(\"The status code for website access is :\",qna_page.status_code)\n",
    "        ######## STORE LAPTOP REVIEWS LOCALLY ############\n",
    "        with open(f\"NVIDIAQNA_{random.randint(0,10000)}.htm\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(qna_page.text)        \n",
    "        qna_soup = BeautifulSoup(qna_page.text, 'html.parser')\n",
    "\n",
    "        All_QNA_Soups.append(qna_soup)\n",
    "\n",
    "        pagination_link = qna_soup.select_one('ul.a-pagination li.a-last a')       \n",
    "\n",
    "        if pagination_link is None:\n",
    "            print('done')\n",
    "            return All_QNA_Soups         \n",
    "\n",
    "        next_url = \"https://www.amazon.com/\" + pagination_link['href']\n",
    "        print(next_url)\n",
    "\n",
    "        if next_url == qa_url:\n",
    "            return All_QNA_Soups\n",
    "\n",
    "        time.sleep(3)\n",
    "        return get_all_qnas_soups(next_url, All_QNA_Soups)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_questions(qna_soup):\n",
    "    \n",
    "    if qna_soup is not None:\n",
    "        Questions = []\n",
    "\n",
    "        questions = qna_soup.select('div.a-section.askTeaserQuestions span.a-declarative')\n",
    "        for q in questions:\n",
    "            Questions.append(q.text.strip())\n",
    "\n",
    "        return Questions\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "###################################################################################################### \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43390301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
